{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f381ad",
   "metadata": {},
   "source": [
    "# Analysis on the loss lanscape and dynamics of optimization for training a neural network\n",
    "- author: hayley song\n",
    "- date: 2022-02-11 (sat)\n",
    "- context: hw1 for cs669 2022sp\n",
    "\n",
    "## Goals\n",
    "- visualize 2d projection of the loss landscape and optimization trajectory during training a neural network.\n",
    "- experiment with model/training hyperparams (e.g. model architecture, dataset, training protocols, choice of optimizer) and report their effects on the loss landscape or the trajectory during loss optimization\n",
    "\n",
    "## Deliverables\n",
    "- A write-up: an abstract, a description of the exp. set-up, resutls, a short discussion. Must constain at minimum:\n",
    "  - [ ] a plot of train and test loss\n",
    "  - [ ] a 2d contour plot of the loss landscape around the optimum\n",
    "  - [ ] a plot of the parameter dynamics in the same 2d projection\n",
    "  - [ ] do the three above for at least two settings: (null, variation)\n",
    "- Code\n",
    "  - well-organized, commented, and\n",
    "  - reproduciable\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ca4ca",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- #[[Qs]] can actually measure the effect of a change in the parameters as the size of gradieent\n",
    "- #[[Qs]] can visualize the effect of a change in the parameters on the output, but this  is done at the inference time, meaning we do this analysis on an already-trained model, so it's not about the learning dynamics, but more on the effect/impact of each model weight variable on the output, through a fixed trained model\n",
    "\n",
    "## Action items:\n",
    "- [ ] fix a design of the model and training protocol\n",
    "  - As a starting point:\n",
    "    - turn off normalization layers, and\n",
    "    - use a smooth activation function that is not ReLU\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "- [ ] build the visualization module\n",
    "  - use FD from Umag's repo\n",
    "  - at the end of a forward step, call FD -> get two vectors, each of length = number of all params\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "- [ ] make a referece section  and put the bibtxts\n",
    "- [ ] table of the parameters: categorized into three broad groups,\n",
    "  - data type\n",
    "  - NN architeuture choices\n",
    "    - discrete\n",
    "      - normalization layer type:\n",
    "      - etc...\n",
    "    - continuous\n",
    "      - number of layers:\n",
    "      - etc...\n",
    "      \n",
    "  - Training objective\n",
    "    - type of regularization: none, l1 (TV),  l2, ...?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426d285",
   "metadata": {},
   "source": [
    "1. Visualize our loss function (ie. the objective function for our optimization \n",
    "problem used for training a NN) over the process of optimization steps\n",
    "The domain of this loss function is the space of all tweaks involved in the training\n",
    "that we are interested in studying its effect on the training process, such as:\n",
    "\n",
    "I will refer to these tweaks as parameters of the loss function. I can categorize \n",
    "them into the following categories, based on the literature in Neural Architecture Search(NAS)[todo: cites] and Auto-ML [todo: cites]. I will closely follow the \n",
    "categorization in [reverse-enginnering] for the model hyperparameters and training type.\n",
    "\n",
    "\n",
    "- Model \n",
    "- $$\\theta$$: weights of the neural network\n",
    "- hyperparameters of the neural network architecuture\n",
    "  - discrete variables:\n",
    "    - type of the layers: fully-connected (FC) vs. convolutional (Conv)\n",
    "    - number of layers: \n",
    "    - \n",
    "- Types of loss function (? not sure if this is relevant here)\n",
    "\n",
    "\n",
    "- b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033dd4e",
   "metadata": {},
   "source": [
    "For visualization of the learning dynamics: \n",
    "- project the parameters into a 2D space using the frequent directions method (Ghashami et al, 2016)\n",
    "  \n",
    "What does FD algorithm do?\n",
    "- computes the top eigenvectors of SVD of the gradients, ie.  \n",
    "  $ SVD (\\nabla_{\\theta}J^{(t)}) $\n",
    "  where $J$ is the loss function/training objective, and $t$ is the index of the \n",
    "  optimization step\n",
    "- at each call, we get two vectors $\\vec{u_1}, \\vec{u_2}$ that can be viewed as \n",
    "the top 2 main directions of the change of the loss function \n",
    "  - these two vectors are conceptually similar to the top 2 principle axis (which \n",
    "  are computationally expensive/infeasible to compute in the high-dim space, e.g. \n",
    "  the domain contains model weights (and variables regarding architectural, training\n",
    "  choices -- but the model weights contribute the most to the cardinality of the \n",
    "  domain)\n",
    "  \n",
    "  \n",
    "Steps:\n",
    "- use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76936c",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9071e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3844cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "from IPython.core.debugger import set_trace as breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf112d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any,List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933ede91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923a580",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import ReprLearn and TileMani packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0de531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reprlearn as rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01ebf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprlearn.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from reprlearn.utils.misc import info, now2str, today2str, get_next_version_path, n_iter_per_epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
